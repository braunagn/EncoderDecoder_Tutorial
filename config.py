# tokenizer and model settings

import torch

####################
### model params ###
####################

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# architecture
C = 512
T = 30  # max context length; informed via quick analysis
N_LAYERS = 6
NUM_HEADS = 8
HEAD_SIZE = 64  # C // NUM_HEADS = 512 // 8

# training
DROPOUT = 0.1
BATCH_SIZE = 8
BATCH_SIZE_VAL = 50
EPOCS = 15
INITIAL_LR = 1e-7
MAX_LR = 1e-5
FINAL_LR = 1e-6
WARM_UP_STEPS = 1000


###########################
### vocab and tokenizer ###
###########################

SPECIAL_TOKENS = {
    "BOS_TOKEN": "<s>",
    "EOS_TOKEN": "</s>",
    "PAD_TOKEN": "[PAD]",
    "UNK_TOKEN": "[UNK]",
}
VOCAB_SIZE = 30000
IGNORE = [
    # chars that appear very infrequently (~1-5 times) in the dataset.  Ignoring these sentence
    # all together given negligible impact on training and project focus is educational
    "Â°",
    "Â²",
    "Â½",
    "Ã",
    "Ã‡",
    "Ã‰",
    "Ã—",
    "ÃŸ",
    "Ã ",
    "Ã¡",
    "Ã¢",
    "Ã£",
    "Ã¤",
    "Ã¥",
    "Ã§",
    "Ã¨",
    "Ã©",
    "Ãª",
    "Ã«",
    "Ã¬",
    "Ã­",
    "Ã®",
    "Ã¯",
    "Ã°",
    "Ã±",
    "Ã³",
    "Ã´",
    "Ã¶",
    "Ãº",
    "Ã»",
    "Ã¼",
    "Ä",
    "Äƒ",
    "Äˆ",
    "Ä‰",
    "ÄŒ",
    "Ä",
    "Ä¥",
    "Ä«",
    "Ä±",
    "Äµ",
    "Å‚",
    "Å",
    "Å",
    "ÅŸ",
    "Å¡",
    "Å­",
    "È™",
    "É™",
    "Ê»",
    "Ï€",
    "á¸¥",
    "á¹›",
    "/",
    "â€¦",
    "âˆš",
    "ğŸŒ¡",
    "ğŸ˜·",
    "ğŸ¤’",
    "ğŸ¤§",
    "ğŸ¤®",
    "ğŸ¦ ",
    "ğŸ§¼",
    "Â«",
    "Â»",
    "Ã",
    "Ã–",
    "Ä‡",
    "Å„",
    "Å",
    "Å«",
    "Ğœ",
    "Ğ§",
    "Ğ°",
    "Ğ·",
    "Ğ¸",
    "Ğº",
    "Ğ»",
    "Ğ¾",
    "Ñ€",
    "Ñ",
    "Ñ‚",
    "Ñ‹",
    "Ñ",
    "×",
    "â€",
    "â€“",
    "â€”",
    "â€˜",
    "â€™",
    "â€œ",
    "â€",
    "â‚‚",
    "â‚¬",
    "â†’",
    "ã‚",
    "@",
    "^",
    "+",
    '"',
    "&",
    "_",
    "{",
    "}",
    "(",
    ")",
    "[",
    "]",
    "#",
    "...",
]
