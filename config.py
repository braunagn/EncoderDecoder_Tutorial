# tokenizer and model settings

import torch

####################
### model params ###
####################

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
T = 30  # max context length; informed via quick analysis
BATCH_SIZE = 8
BATCH_SIZE_VAL = 50


###########################
### vocab and tokenizer ###
###########################

SPECIAL_TOKENS = {
    "BOS_TOKEN": "<s>",
    "EOS_TOKEN": "</s>",
    "PAD_TOKEN": "[PAD]",
    "UNK_TOKEN": "[UNK]",
}
VOCAB_SIZE = 30000
IGNORE = [
    # chars that appear very infrequently (1-5 times) in the dataset.  Ignoring these sentence
    # all together given negligible impact on training and project focus is educational
    'Â°', 'Â²', 'Â½', 'Ã', 'Ã‡', 'Ã‰', 'Ã—', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯',
    'Ã°', 'Ã±', 'Ã³', 'Ã´', 'Ã¶', 'Ãº', 'Ã»', 'Ã¼', 'Ä', 'Äƒ', 'Äˆ', 'Ä‰', 'ÄŒ', 'Ä', 'Ä¥', 'Ä«', 'Ä±', 'Äµ', 'Å‚', 'Å', 'Å', 'ÅŸ', 'Å¡',
    'Å­', 'È™',  'É™', 'Ê»', 'Ï€', 'á¸¥', 'á¹›',  '/', 'â€¦', 'âˆš', 'ğŸŒ¡', 'ğŸ˜·', 'ğŸ¤’', 'ğŸ¤§', 'ğŸ¤®', 'ğŸ¦ ',  'ğŸ§¼', 'Â«', 'Â»', 'Ã',
    'Ã–', 'Ä‡', 'Å„', 'Å', 'Å«', 'Ğœ', 'Ğ§', 'Ğ°', 'Ğ·', 'Ğ¸', 'Ğº', 'Ğ»', 'Ğ¾', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñ‹', 'Ñ', '×', 'â€', 'â€“', 'â€”', 'â€˜',
    'â€™', 'â€œ', 'â€', 'â‚‚', 'â‚¬', 'â†’', 'ã‚', '@', '^', '+', '"', '&', '_', '{', '}', '(', ')', '[', ']', '#', "...",
  ]